<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>MVControl: Adding Conditional Control to Multi-view Diffusion for Controllable Text-to-3D Generation</title>
	<link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico">
    <meta content="MVControl: Adding Conditional Control to Multi-view Diffusion for Controllable Text-to-3D Generation" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MLDP9MKGC8');
    </script>
</head>

<body>
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">MVControl: Adding Conditional Control to Multi-view Diffusion for Controllable Text-to-3D Generation</h1>
            <div class="nerf_subheader_v2">CVPR 2024</div>
            <div class="nerf_subheader_v2">
                Anonymous authors
                <!-- <div>
                    <a href="https://mrtornado24.github.io/" target="_blank" class="nerf_authors_v2">Jingxiang Sun<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://bo-zhang.me/" target="_blank" class="nerf_authors_v2">Bo Zhang<span
                            class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;
                    <a href="https://dsaurus.github.io/saurus/" target="_blank" class="nerf_authors_v2">Ruizhi Shao<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://lizhenwangt.github.io/" target="_blank" class="nerf_authors_v2">Lizhen Wang<span
                        class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://github.com/StevenLiuWen" target="_blank" class="nerf_authors_v2">Wen Liu<span
                        class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://zdaxie.github.io/" target="_blank" class="nerf_authors_v2">Zhenda Xie<span
                        class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://liuyebin.com/" target="_blank" class="nerf_authors_v2">Yebin Liu<span
                            class="text-span_nerf"></span></a><sup> 1</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>Tsinghua University</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>2 </sup>DeepSeek AI</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>Independent Researcher</h1>
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/abs/2310.16818" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <a class="btn" href="paper/Paper_high_res.pdf" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a>
                    <a class="btn" href="https://github.com/deepseek-ai/DreamCraft3D" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-github"></i> Code </a>
                    <a class="btn btn-large btn-light" href="https://youtu.be/0FazXENkQms" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-youtube"></i> Video </a>
                </div> -->

            </div>
        </div>

    </div>


    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                We introduce MVControl, a novel neural network architecture that enhances existing pre-trained multi-view 2D diffusion models by incorporating additional input 
                conditions, e.g. edge maps. Our approach enables the generation of controllable multi-view images and view-consistent 3D content. To achieve controllable multi-view 
                image generation, we leverage MVDream as our base model, and train a new neural network module as additional plugin for end-to-end task-specific condition learning. 
                To precisely control the shapes and views of generated images, we innovatively propose a new conditioning mechanism that predicts an embedding encapsulating the input 
                spatial and view conditions, which is then injected to the network globally. Once MVControl is trained, score-distillation (SDS) loss based optimization can be performed 
                to generate 3D content, in which process we propose to use a hybrid diffusion prior. The hybrid prior relies on a pre-trained Stable-Diffusion network and our trained 
                MVControl for additional guidance. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 
                3D content.
                <br>
                <!-- <img  src="assets/images/overview.png"> -->
            </p>
        </div>
    </div>


    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Generated Textured Meshes</h2>
        <div class="grid-container-1">
            <div>
                <p class="myprompt nerf_text">
                    A DSLR photo of a man, middle hear with bangs, handsome, 24 years old, casual dressed, hands in pockets, 
                    extremely detailed
                </p>
                <video class="video" loop playsinline autoPlay muted src="assets_mvcontrol/videos/man.mp4"></video>
            </div>
            <div>
                <p class="myprompt nerf_text">
                    An astronaut, full body, extremely detailed spacesuit, dynamic shot, walking pose, best quality 
                </p>
                <video class="video" loop playsinline autoPlay muted src="assets_mvcontrol/videos/astronaut.mp4"></video>
            </div>
            <div>
                <p class="myprompt nerf_text">
                    Photorealistic photo of the male witcher, upper body, middle aged, long white hair, Medieval black light 
                    armor,a sword on the back, perfect face, scar, necklace in the shape of a wolf, detailed face and body
                </p>
            <video class="video" loop playsinline autoPlay muted src="assets_mvcontrol/videos/witcher.mp4"></video>
            </div>
            <div>
                <p class="myprompt nerf_text">
                    1girl, upper body, perfect face, looking straight ahead, photorealistic highly detailed 8k photography, 
                    young woman in rwtcdress, extremely detailed 
                </p>
                <video class="video" loop playsinline autoPlay muted src="assets_mvcontrol/videos/girl16.mp4"></video>
            </div>
            <div>
                <p class="myprompt nerf_text">
                    BJ_Sacred_beast, Drawing, film grain, Belted Kingfisher in temperate ocean,
                    standing on a branch, masterpiece, best quality, highest details, 4k, 8k </p>
                <video class="video" loop playsinline autoPlay muted src="assets_mvcontrol/videos/bird.mp4"></video>
            </div>
            <div>
                <p class="myprompt nerf_text">
                    a vanilla coffee ice cream cone with whipped cream, colorful,
                    delicious, digital art, best quality, highest details, 8k.
                </p>
                <video class="video" loop playsinline autoPlay muted src="assets_mvcontrol/videos/icecream.mp4"></video>
            </div>
        </div>
        <div class="grid-container-1">
            <a class="mybtn" href="dmtet-based-gallery_0.html" role="button">
             More Results </a>
        </div>
    </div>

    

    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Method Overview</h2>
        <div class="grid-container-1">
            <img src="assets_mvcontrol/images/overview_mvcontrol.png">

            <p>Overview of proposed method. (a) MVControl consists of a frozen multi-view diffusion model and a trainable MVControl. (b)
                Our model takes care of all input conditions to control the generation process both locally and globally through a conditioning module.
                (c) Once MVControl is trained, we can exploit it to serve a hybrid diffusion prior for controllable text-to-3D content generation via SDS
                optimization procedure.
            </p>
        </div>
    </div>

<!-- <div class="white_section_nerf grey_container w-container">
<h2 class="grey-heading_nerf">BibTeX</h2>
<div class="bibtex">
    <pre><code>@misc{sun2023dreamcraft3d,
        title={DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior}, 
        author={Jingxiang Sun and Bo Zhang and Ruizhi Shao and Lizhen Wang and Wen Liu and Zhenda Xie and Yebin Liu},
        year={2023},
        eprint={2310.16818},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
  }</code></pre>
</div>
</div> -->

</body>
<footer>
    This project page is inspired by <a href="https://mrtornado24.github.io/DreamCraft3D/">DreamCraft3D</a>.
</footer>

</html>
